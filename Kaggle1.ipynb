{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kaggle1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Vm6lBw8ZhdU0",
        "ul9wwYybApeE",
        "z8-A3vXmg-7b",
        "1a2_79I5hGHl",
        "1iNnt1etr0MU",
        "hJiNBNtbsBKB"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK6Pz_dQBiY7"
      },
      "source": [
        "# First Kaggle competition - Image classification\n",
        "\n",
        "To complete this challenge I opted to start with a CNN inspired on VGG-11 simplifying the FC layers at the top thus reducing the number of parameters (the majority resides in the classification layers) and so the tendency to overfit.\n",
        "\n",
        "The results are encouraging but I don't think that is possible to obtain much more from a network trained from scratch due to the small dataset.\n",
        "So my next try will use transfer learning and fine tuning.\n",
        "\n",
        "I used a VGG-16 already trained with imagenet dataset and I appended at the end another convolutional layer plus a dense net to perform classification.\n",
        "With TL and some hyperparameter tuning the model was able to reach a score of 0.89333 on the test set. I think the model can be improved by analyzing the activations in the last layer of the network to understand which other layers to include, remove or modify.\n",
        "\n",
        "My last try was with a unfreezed Xception network, the classification layer that I added at the end is similar to the one used in previous try but now I've let all the network's layers to be trained.\n",
        "Obviously I had to increase the learning rate so I opted to use also the ReduceLROnPlateau callback to better approach the local minimum.\n",
        "This model was able to achieve a score of 0.92222.\n",
        "\n",
        "Unfortunately I was not able to find a teammate because I started some days after the publication of the challenge.\n",
        "\n",
        "During this challenge I used, instead of TensorBoard, Weight and Biases to have statistics about how the model is performing and to perform hyperparameters tuning in a simpler way. Here you can find the entire project with all the runs and sweeps: https://wandb.ai/lrsb/kaggle1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm6lBw8ZhdU0"
      },
      "source": [
        "# Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmvzPMcS0Y26"
      },
      "source": [
        "import json\n",
        "\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "!pip install --upgrade wandb\n",
        "\n",
        "#@markdown Insert here your credentials\n",
        "kaggle_username = ''#@param {type:'string'}\n",
        "kaggle_api_key = ''#@param {type:'string'}\n",
        "wandb_key = ''#@param {type:'string'}\n",
        "\n",
        "!wandb login {wandb_key}\n",
        "\n",
        "api_token = {'username': kaggle_username, 'key': kaggle_api_key}\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as kaggle_json:\n",
        "  json.dump(api_token, kaggle_json)\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c artificial-neural-networks-and-deep-learning-2020\n",
        "!unzip -q artificial-neural-networks-and-deep-learning-2020.zip\n",
        "\n",
        "!mkdir ./MaskDataset/train ./MaskDataset/train/0 ./MaskDataset/train/1 ./MaskDataset/train/2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul9wwYybApeE"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlY4iPY7A4nG"
      },
      "source": [
        "### Splitting dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXSG4IVaA4MK"
      },
      "source": [
        "import json, shutil, os\n",
        "\n",
        "cwd = os.getcwd()\n",
        "\n",
        "dataset_dir = os.path.join(cwd, 'MaskDataset')\n",
        "training_dir = os.path.join(dataset_dir, 'training')\n",
        "split_dir = os.path.join(dataset_dir, 'train')\n",
        "\n",
        "with open(os.path.join(dataset_dir, 'train_gt.json')) as js:\n",
        "  data = json.load(js)\n",
        "  files = os.listdir(training_dir)\n",
        "\n",
        "  for f in files:\n",
        "    start_dir = os.path.join(training_dir, f)\n",
        "    end_dir = os.path.join(os.path.join(split_dir, str(data[f])), f)\n",
        "    shutil.move(start_dir, end_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKtqKkKnhg5O"
      },
      "source": [
        "### Making results more reproducible and setting params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrQ-l7ufA_0B"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "SEED = 1234#@param {type:'number'}\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "num_classes = 3\n",
        "classes = ['none', 'all', 'some']\n",
        "\n",
        "img_w = 299#@param {type:'number'}\n",
        "img_h = 299#@param {type:'number'}\n",
        "\n",
        "input_shape = (img_h, img_w, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOfFzQxzhm0i"
      },
      "source": [
        "###Â Code for creating datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTNuqLgITkC4"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def GetDatasets(data_augmentation, validation_split, batch_size, preprocessing_function):\n",
        "  if data_augmentation:\n",
        "    image_generator = ImageDataGenerator(rotation_range=30,\n",
        "                                        width_shift_range=0.1,\n",
        "                                        height_shift_range=0.1,\n",
        "                                        zoom_range=0.2,\n",
        "                                        horizontal_flip=True,\n",
        "                                        vertical_flip=False,\n",
        "                                        rescale=1./255,\n",
        "                                        validation_split=validation_split,\n",
        "                                        preprocessing_function=preprocessing_function)\n",
        "  else:\n",
        "    image_generator = ImageDataGenerator(rescale=1./255,\n",
        "                                        validation_split=validation_split,\n",
        "                                        preprocessing_function=preprocessing_function)\n",
        "    \n",
        "  test_img_generator = ImageDataGenerator(rescale=1./255, preprocessing_function=preprocessing_function)\n",
        "    \n",
        "  train_gen = image_generator.flow_from_directory(split_dir,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  target_size=(img_h, img_w),\n",
        "                                                  color_mode='rgb',\n",
        "                                                  class_mode='categorical',\n",
        "                                                  shuffle=True,\n",
        "                                                  subset='training',\n",
        "                                                  seed=SEED)\n",
        "\n",
        "  valid_gen = image_generator.flow_from_directory(split_dir,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  target_size=(img_h, img_w),\n",
        "                                                  color_mode='rgb',\n",
        "                                                  class_mode='categorical',\n",
        "                                                  shuffle=True,\n",
        "                                                  subset='validation',\n",
        "                                                  seed=SEED)\n",
        "\n",
        "  test_gen = test_img_generator.flow_from_directory(dataset_dir,\n",
        "                                                    batch_size=1,\n",
        "                                                    target_size=(img_h, img_w),\n",
        "                                                    color_mode='rgb',\n",
        "                                                    classes=['test'],\n",
        "                                                    shuffle=False,\n",
        "                                                    seed=SEED)\n",
        "\n",
        "  train_dataset = tf.data.Dataset.from_generator(lambda: train_gen,\n",
        "                                                output_types=(tf.float32, tf.float32),\n",
        "                                                output_shapes=([None, img_w, img_h, 3], [None, num_classes]))\n",
        "  train_dataset = train_dataset.repeat()\n",
        "\n",
        "\n",
        "  valid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen, \n",
        "                                                output_types=(tf.float32, tf.float32),\n",
        "                                                output_shapes=([None, img_w, img_h, 3], [None, num_classes]))\n",
        "  valid_dataset = valid_dataset.repeat()\n",
        "\n",
        "\n",
        "  test_dataset = tf.data.Dataset.from_generator(lambda: test_gen,\n",
        "                                                output_types=(tf.float32, tf.float32),\n",
        "                                                output_shapes=([None, img_w, img_h, 3], [None, num_classes]))\n",
        "  test_dataset = test_dataset.repeat()\n",
        "\n",
        "  return train_dataset, train_gen, valid_dataset, valid_gen, test_dataset, test_gen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_oM46Q3hsF9"
      },
      "source": [
        "### Code for saving testset results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ba1WBpQawed"
      },
      "source": [
        "import ntpath\n",
        "\n",
        "def PredictDataset(model, dataset, directory):\n",
        "  predictions = model.predict_generator(dataset, len(dataset), verbose=1)\n",
        "  filenames = dataset.filenames\n",
        "\n",
        "  results = {}\n",
        "  i = 0\n",
        "  for p in predictions:\n",
        "    results[ntpath.basename(filenames[i])] = str(np.argmax(p))\n",
        "    i = i + 1\n",
        "\n",
        "  with open(os.path.join(directory, 'results.csv'), 'w') as f:\n",
        "    f.write('Id,Category\\n')\n",
        "    for key, value in results.items():\n",
        "      f.write(key + ',' + str(value) + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8-A3vXmg-7b"
      },
      "source": [
        "# (1) CNN based on VGG-11 architecture\n",
        "### Kaggle score: 0.81555\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDrZzjfBTY6v"
      },
      "source": [
        "import wandb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.tensorflow.keras.optimizers import RMSprop\n",
        "from wandb.keras import WandbCallback\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "def GetVGG11BasedCNN(learning_rate):\n",
        "  model = Sequential()\n",
        "\n",
        "  # CNN-1: conv3-64 + maxpool\n",
        "  model.add(Convolution2D(64, 3, padding='same', input_shape=input_shape, activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  # CNN-2: conv3-128 + maxpool\n",
        "  model.add(Convolution2D(128, 3, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  # CNN-3: conv3-256x2 + maxpool\n",
        "  model.add(Convolution2D(256, 3, padding='same', activation='relu'))\n",
        "  model.add(Convolution2D(256, 3, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  # CNN-4: conv3-512x2 + maxpool\n",
        "  model.add(Convolution2D(512, 3, padding='same', activation='relu'))\n",
        "  model.add(Convolution2D(512, 3, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  # CNN-5: conv3-512x2 + maxpool\n",
        "  model.add(Convolution2D(512, 3, padding='same', activation='relu'))\n",
        "  model.add(Convolution2D(512, 3, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  # FC-1024x2 Fully connected layers\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(units=1024, activation='relu'))\n",
        "  model.add(Dense(units=1024, activation='relu'))\n",
        "\n",
        "  # FC-3 Last layer\n",
        "  model.add(Dense(units=num_classes, activation='softmax'))\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', \n",
        "                optimizer=RMSprop(learning_rate=learning_rate),\n",
        "                metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "#@markdown Set hyperparameters used during training\n",
        "\n",
        "data_augmentation = True#@param {type:'boolean'}\n",
        "validation_split = 0.1#@param {type:'number'}\n",
        "batch_size = 8#@param {type:'number'}\n",
        "epochs = 100#@param {type:'number'}\n",
        "use_early_stopping = True#@param {type:'boolean'}\n",
        "learning_rate = 1e-5#@param {type:'number'}\n",
        "\n",
        "#@markdown Or use hyperparameter optimization (done on WandB platform)\n",
        "\n",
        "use_hyperparameter_optimization = False#@param {type:'boolean'}\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "if use_hyperparameter_optimization:\n",
        "  def RunFitWithHypOpt():\n",
        "    wandb.init()\n",
        "    datasets = GetDatasets(wandb.config.data_augmentation,\n",
        "                           wandb.config.validation_split,\n",
        "                           wandb.config.batch_size,\n",
        "                           preprocess_input)\n",
        "    model = GetVGG11BasedCNN(wandb.config.learning_rate)\n",
        "    \n",
        "    model.fit(x=datasets[0],\n",
        "              epochs=wandb.config.epochs,\n",
        "              steps_per_epoch=len(datasets[1]),\n",
        "              validation_data=datasets[2],\n",
        "              validation_steps=len(datasets[3]), \n",
        "              callbacks=[EarlyStopping(monitor='val_accuracy', mode='max', patience=8, restore_best_weights=True),\n",
        "                         WandbCallback(data_type='image', labels=classes)])\n",
        "\n",
        "    PredictDataset(model, datasets[5], wandb.run.dir)\n",
        "\n",
        "  # Initialise WandB agent that will perform HP optimization\n",
        "  wandb.agent(wandb.sweep({\n",
        "    'description': 'VGG11 based CNN optimization',\n",
        "    'method': 'bayes',\n",
        "    'metric': {\n",
        "        'name': 'val_accuracy',\n",
        "        'goal': 'maximize'\n",
        "    },\n",
        "    'early_terminate': {\n",
        "        'type': 'hyperband',\n",
        "        'min_iter': 3\n",
        "    },\n",
        "    'parameters': {\n",
        "          'data_augmentation': {\n",
        "              'values': [True, False]\n",
        "          },\n",
        "          'validation_split': {\n",
        "              'values': [0.1, 0.2]\n",
        "          },\n",
        "          'batch_size': {\n",
        "              'values': [4, 8, 16, 32, 64]\n",
        "          },\n",
        "          'epochs': {\n",
        "              'value': 100\n",
        "          },\n",
        "          'learning_rate': {\n",
        "              'values': [1e-5, 5e-5, 1e-4]\n",
        "          }\n",
        "      }\n",
        "  }, project='kaggle1'), function=RunFitWithHypOpt)\n",
        "\n",
        "else:\n",
        "  wandb.init(project='kaggle1', config={\n",
        "      'data_augmentation': data_augmentation,\n",
        "      'validation_split': validation_split,\n",
        "      'batch_size': batch_size,\n",
        "      'epochs': epochs,\n",
        "      'use_early_stopping': use_early_stopping,\n",
        "      'learning_rate': learning_rate\n",
        "  })\n",
        "\n",
        "  callbacks = [WandbCallback(data_type='image', labels=classes)]\n",
        "  if use_early_stopping:\n",
        "    callbacks.append(EarlyStopping(monitor='val_accuracy', mode='max', patience=10, restore_best_weights=True))\n",
        "\n",
        "  datasets = GetDatasets(data_augmentation,\n",
        "                         validation_split,\n",
        "                         batch_size,\n",
        "                         preprocess_input)\n",
        "  model = GetVGG11BasedCNN(learning_rate)\n",
        "  model.summary()\n",
        "  \n",
        "  model.fit(x=datasets[0],\n",
        "            epochs=epochs,\n",
        "            steps_per_epoch=len(datasets[1]),\n",
        "            validation_data=datasets[2],\n",
        "            validation_steps=len(datasets[3]), \n",
        "            callbacks=callbacks)\n",
        "\n",
        "  PredictDataset(model, datasets[5], wandb.run.dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_nfSod4fgJi"
      },
      "source": [
        "### Results\n",
        "\n",
        "Download model [here](https://drive.google.com/file/d/1-50J64ZxbJETPqTCPl8xH7ZALow-uSyo/view?usp=sharing)\n",
        "\n",
        "Details of the run [here](https://wandb.ai/lrsb/kaggle1/runs/3k466bg5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a2_79I5hGHl"
      },
      "source": [
        "# (2) Transfer learning and fine tuning on VGG16 \n",
        "### Kaggle score: 0.89333\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGFV8XTqEmhc"
      },
      "source": [
        "import wandb\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from wandb.keras import WandbCallback\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "def GetModel(learning_rate, filters, units_1, units_2, ft_level):\n",
        "  vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "  for layer in vgg16.layers[:ft_level]:\n",
        "    layer.trainable =  False\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(vgg16)\n",
        "\n",
        "  model.add(Conv2D(filters, 3, padding='valid', activation='relu'))\n",
        "  model.add(Dense(units=units_1, activation='relu'))\n",
        "  model.add(GlobalAveragePooling2D())\n",
        "\n",
        "  model.add(Dense(units=units_2, activation='relu'))\n",
        "  model.add(Dense(units=num_classes, activation='softmax'))\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', \n",
        "                optimizer=Adam(learning_rate),\n",
        "                metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "#@markdown Set hyperparameters used during training\n",
        "\n",
        "data_augmentation = True#@param {type:'boolean'}\n",
        "validation_split = 0.1#@param {type:'number'}\n",
        "batch_size = 8#@param {type:'number'}\n",
        "epochs = 100#@param {type:'number'}\n",
        "use_early_stopping = True#@param {type:'boolean'}\n",
        "learning_rate = 1e-5#@param {type:'number'}\n",
        "filters = 16#@param {type:'number'}\n",
        "units_1 = 128#@param {type:'number'}\n",
        "units_2 = 256#@param {type:'number'}\n",
        "ft_level = 13#@param {type:'number'}\n",
        "\n",
        "#@markdown Or use hyperparameter optimization (done on WandB platform)\n",
        "\n",
        "use_hyperparameter_optimization = False#@param {type:'boolean'}\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "if use_hyperparameter_optimization:\n",
        "  def RunFitWithHypOpt():\n",
        "    wandb.init()\n",
        "    datasets = GetDatasets(wandb.config.data_augmentation,\n",
        "                           wandb.config.validation_split,\n",
        "                           wandb.config.batch_size,\n",
        "                           preprocess_input)\n",
        "    model = GetModel(wandb.config.learning_rate,\n",
        "                     wandb.config.filters,\n",
        "                     wandb.config.units_1,\n",
        "                     wandb.config.units_2,\n",
        "                     wandb.config.ft_level)\n",
        "    \n",
        "    model.fit(x=datasets[0],\n",
        "              epochs=wandb.config.epochs,\n",
        "              steps_per_epoch=len(datasets[1]),\n",
        "              validation_data=datasets[2],\n",
        "              validation_steps=len(datasets[3]), \n",
        "              callbacks=[EarlyStopping(monitor='val_accuracy', mode='max', patience=8, restore_best_weights=True),\n",
        "                         WandbCallback(data_type='image', labels=classes)])\n",
        "\n",
        "    PredictDataset(datasets[5], wandb.run.dir)\n",
        "\n",
        "  # Initialise WandB agent that will perform HP optimization\n",
        "  wandb.agent(wandb.sweep({\n",
        "    'description': 'VGG16 TL optimization',\n",
        "    'method': 'bayes',\n",
        "    'metric': {\n",
        "        'name': 'val_accuracy',\n",
        "        'goal': 'maximize'\n",
        "    },\n",
        "    'early_terminate': {\n",
        "        'type': 'hyperband',\n",
        "        'min_iter': 3\n",
        "    },\n",
        "    'parameters': {\n",
        "          'data_augmentation': {\n",
        "              'values': [True, False]\n",
        "          },\n",
        "          'validation_split': {\n",
        "              'values': [0.1, 0.2]\n",
        "          },\n",
        "          'batch_size': {\n",
        "              'values': [4, 8, 16, 32, 64]\n",
        "          },\n",
        "          'epochs': {\n",
        "              'value': 50\n",
        "          },\n",
        "          'learning_rate': {\n",
        "              'values': [1e-5, 5e-5, 1e-4]\n",
        "          },\n",
        "          'filter': {\n",
        "              'values': [16, 32, 64]\n",
        "          },\n",
        "          'units_1': {\n",
        "              'values': [128, 256, 512]\n",
        "          },\n",
        "          'units_2': {\n",
        "              'values': [64, 126, 256]\n",
        "          },\n",
        "          'ft_level': {\n",
        "              'values': [13, 14]\n",
        "          }\n",
        "      }\n",
        "  }, project='kaggle1'), function=RunFitWithHypOpt)\n",
        "\n",
        "else:\n",
        "  wandb.init(project='kaggle1', config={\n",
        "      'data_augmentation': data_augmentation,\n",
        "      'validation_split': validation_split,\n",
        "      'batch_size': batch_size,\n",
        "      'epochs': epochs,\n",
        "      'use_early_stopping': use_early_stopping,\n",
        "      'learning_rate': learning_rate,\n",
        "      'filter': filter,\n",
        "      'units_1': units_1,\n",
        "      'units_2': units_2,\n",
        "      'ft_level': ft_level\n",
        "  })\n",
        "\n",
        "  callbacks = [WandbCallback(data_type='image', labels=classes)]\n",
        "  if use_early_stopping:\n",
        "    callbacks.append(EarlyStopping(monitor='val_accuracy', mode='max', patience=10, restore_best_weights=True))\n",
        "\n",
        "  datasets = GetDatasets(data_augmentation,\n",
        "                         validation_split,\n",
        "                         batch_size,\n",
        "                         preprocess_input)\n",
        "  model = GetModel(learning_rate,\n",
        "                   filters,\n",
        "                   units_1,\n",
        "                   units_2,\n",
        "                   ft_level)\n",
        "  \n",
        "  model.fit(x=datasets[0],\n",
        "            epochs=epochs,\n",
        "            steps_per_epoch=len(datasets[1]),\n",
        "            validation_data=datasets[2],\n",
        "            validation_steps=len(datasets[3]), \n",
        "            callbacks=callbacks)\n",
        "\n",
        "  PredictDataset(datasets[5], wandb.run.dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAKslCjf6qoj"
      },
      "source": [
        "### Results\n",
        "\n",
        "Download model [here](https://drive.google.com/file/d/1gSlj2il_R2kc0F_3aWZildpUS9gKgToJ/view?usp=sharing)\n",
        "\n",
        "Details of the run [here](https://wandb.ai/lrsb/kaggle1/runs/o6vogtpx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iNnt1etr0MU"
      },
      "source": [
        "# (3) Transfer learning and retuning of Xception with ReduceLROnPlateau\n",
        "### Kaggle score: 0.92222"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2gbNFIqeUoG"
      },
      "source": [
        "import wandb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, GlobalAveragePooling2D, BatchNormalization\n",
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from wandb.keras import WandbCallback\n",
        "from tensorflow.keras.applications.xception import preprocess_input\n",
        "\n",
        "def GetModel(learning_rate):\n",
        "  xcp = Xception(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(xcp)\n",
        "  model.add(Conv2D(32, 3, padding='valid', activation='relu'))\n",
        "  model.add(Conv2D(32, 3, padding='valid', activation='relu'))\n",
        "  model.add(Dense(units=256, activation='relu'))\n",
        "  model.add(GlobalAveragePooling2D())\n",
        "\n",
        "  model.add(Dense(units=128, activation='relu'))\n",
        "  model.add(Dense(units=num_classes, activation='softmax'))\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=Adam(learning_rate),\n",
        "                metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "#@markdown #MODIFY DATASET SHAPE TO 299x299 TO FIT XCEPTION INPUT SHAPE (SETUP SECTION)\n",
        "\n",
        "#@markdown Set hyperparameters used during training\n",
        "\n",
        "data_augmentation = True#@param {type:'boolean'}\n",
        "validation_split = 0.1#@param {type:'number'}\n",
        "batch_size = 32#@param {type:'number'}\n",
        "epochs = 50#@param {type:'number'}\n",
        "use_early_stopping = True#@param {type:'boolean'}\n",
        "learning_rate = 1e-3#@param {type:'number'}\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "wandb.init(project='kaggle1', config={\n",
        "    'data_augmentation': data_augmentation,\n",
        "    'validation_split': validation_split,\n",
        "    'batch_size': batch_size,\n",
        "    'epochs': epochs,\n",
        "    'use_early_stopping': use_early_stopping,\n",
        "    'learning_rate': learning_rate\n",
        "})\n",
        "\n",
        "callbacks = [WandbCallback(data_type='image', labels=classes)]\n",
        "\n",
        "if use_early_stopping:\n",
        "  callbacks.append(EarlyStopping(monitor='val_accuracy',\n",
        "                                 mode='max',\n",
        "                                 verbose=1,\n",
        "                                 patience=8,\n",
        "                                 restore_best_weights=True))\n",
        "  \n",
        "callbacks.append(ReduceLROnPlateau(monitor='val_accuracy',\n",
        "                                   mode='max',\n",
        "                                   factor=0.1,\n",
        "                                   patience=3,\n",
        "                                   cooldown=1,\n",
        "                                   min_delta=0,\n",
        "                                   verbose=1))\n",
        "\n",
        "datasets = GetDatasets(data_augmentation,\n",
        "                        validation_split,\n",
        "                        batch_size,\n",
        "                        preprocess_input)\n",
        "model = GetModel(learning_rate)\n",
        "\n",
        "model.fit(x=datasets[0],\n",
        "          epochs=epochs,\n",
        "          steps_per_epoch=len(datasets[1]),\n",
        "          validation_data=datasets[2],\n",
        "          validation_steps=len(datasets[3]), \n",
        "          callbacks=callbacks)\n",
        "\n",
        "PredictDataset(model, datasets[5], wandb.run.dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t06stx948okk"
      },
      "source": [
        "### Results\n",
        "\n",
        "Download model [here](https://drive.google.com/file/d/1gSlj2il_R2kc0F_3aWZildpUS9gKgToJ/view?usp=sharing)\n",
        "\n",
        "Details of the run [here](https://wandb.ai/lrsb/kaggle1/runs/age3svuk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJiNBNtbsBKB"
      },
      "source": [
        "# Utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgnQ54_fnfTI"
      },
      "source": [
        "### Predict dataset using a saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gj8w73qdi9Md"
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "model = keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/Kaggle1/vgg16-model-best.h5')\n",
        "\n",
        "datasets = GetDatasets(True, 0.1, 8, keras.applications.vgg16.preprocess_input)\n",
        "\n",
        "PredictDataset(model, datasets[5], '/content')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}