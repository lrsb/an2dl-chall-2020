{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CodaLab2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Vm6lBw8ZhdU0",
        "ul9wwYybApeE",
        "z8-A3vXmg-7b",
        "hJiNBNtbsBKB"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK6Pz_dQBiY7"
      },
      "source": [
        "# Second CodaLab competition - Image segmentation\n",
        "\n",
        "Model finetuning:\n",
        "\n",
        "To better tune this model with the new set (previously the testset) I trained the previous model (the best performing one) with the last learining rate selected by ReduceLROnPlateau.\n",
        "\n",
        "Previous description:\n",
        "\n",
        "For this challenge I have chosen to train the model only on the Bipbip Haricot dataset as, to predict the other dataset, it is only necessary to retrain the model on the other dataset.\n",
        "Some image preprocessing has to be done on the Pead dataset (perispective transformation to enlarge the farthest elements) and on the Roseau dataset (image color normalization).\n",
        "\n",
        "I started with a VGG-16 as a encoder layer and a decoder layer made of UpSampling2D and Conv2D (as in the excercise session example).\n",
        "\n",
        "I trained this model with data augmentation and hyperparameter tuning (depth and filters of the decoder) but the results were not so encouraging so I opted for a U-Net architecture.\n",
        "\n",
        "I tried two main models one implemented in the library https://github.com/karolzak/keras-unet.\n",
        "\n",
        "During the trials I've tried different techniques to achiveve the maximum IoU on the test set, for example splitting the the image in different patches: 12 of size (512x512) but the results were not so different from the traditional approach, I tried also to train the model on a single class and then merge the predictions using argmax.\n",
        "\n",
        "The main problem with the custom U-Net was that different classes were predicted as one, but with a pretty high IoU over both classes.\n",
        "\n",
        "Finally I've chosen to use relatively high LR with ReduceLROnPlateau, high number of filters per conv layer and use BatchNormalization layers.\n",
        "\n",
        "During this challenge I used, instead of TensorBoard, Weight and Biases to have statistics about how the model is performing and to perform hyperparameters tuning in a simpler way. Here you can find the entire project with all the runs: https://wandb.ai/lrsb/codalab2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm6lBw8ZhdU0"
      },
      "source": [
        "# Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmvzPMcS0Y26"
      },
      "source": [
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "!pip install --upgrade wandb\n",
        "!pip install keras-unet\n",
        "\n",
        "#@markdown Insert here your credentials\n",
        "wandb_key = ''#@param {type:'string'}\n",
        "\n",
        "!wandb login {wandb_key}\n",
        "\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "!echo \"Copying dataset...\"\n",
        "!cp '/content/drive/MyDrive/Colab Notebooks/CodaLab2/Development_Dataset.zip' '/content/Development_Dataset.zip'\n",
        "!cp '/content/drive/MyDrive/Colab Notebooks/CodaLab2/Final_Dataset.zip' '/content/Final_Dataset.zip'\n",
        "!echo \"Extracting dataset...\"\n",
        "!unzip -q /content/Development_Dataset.zip\n",
        "!unzip -q /content/Final_Dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul9wwYybApeE"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKtqKkKnhg5O"
      },
      "source": [
        "### Making results more reproducible and setting params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrQ-l7ufA_0B"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "SEED = 1234#@param {type:'number'}\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "img_w = 1024#@param {type:'number'}\n",
        "img_h = 1024#@param {type:'number'}\n",
        "\n",
        "input_shape = (img_h, img_w, 3)\n",
        "\n",
        "num_classes = 3\n",
        "class_names = ['background', 'crop', 'weed']\n",
        "class_colors = [[0, 0, 0], [255, 255, 255], [216, 67, 82]]\n",
        "\n",
        "teams = ['Bipbip', 'Pead', 'Roseau', 'Weedelec']\n",
        "crop_types = ['Haricot', 'Mais']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w25a1dzUjxf"
      },
      "source": [
        "### Supporting functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFouUld4Tlsh"
      },
      "source": [
        "def rle_encode(img):\n",
        "  '''\n",
        "  img: numpy array, 1 - foreground, 0 - background\n",
        "  Returns run length as string formatted\n",
        "  '''\n",
        "  pixels = img.flatten()\n",
        "  pixels = np.concatenate([[0], pixels, [0]])\n",
        "  runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
        "  runs[1::2] -= runs[::2]\n",
        "  return ' '.join(str(x) for x in runs)\n",
        "\n",
        "def rle_decode(rle, shape):\n",
        "  s = rle.split()\n",
        "  starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
        "  starts -= 1\n",
        "  ends = starts + lengths\n",
        "  img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
        "  for lo, hi in zip(starts, ends):\n",
        "      img[lo:hi] = 1\n",
        "  return img.reshape(shape)\n",
        "\n",
        "def get_patches(img_arr, size=256, stride=256):\n",
        "    \"\"\"\n",
        "    Takes single image or array of images and returns\n",
        "    crops using sliding window method.\n",
        "    If stride < size it will do overlapping.\n",
        "    \n",
        "    Args:\n",
        "        img_arr (numpy.ndarray): [description]\n",
        "        size (int, optional): [description]. Defaults to 256.\n",
        "        stride (int, optional): [description]. Defaults to 256.\n",
        "    \n",
        "    Raises:\n",
        "        ValueError: [description]\n",
        "        ValueError: [description]\n",
        "    \n",
        "    Returns:\n",
        "        numpy.ndarray: [description]\n",
        "    \"\"\"    \n",
        "    # check size and stride\n",
        "    if size % stride != 0:\n",
        "        raise ValueError(\"size % stride must be equal 0\")\n",
        "\n",
        "    patches_list = []\n",
        "    overlapping = 0\n",
        "    if stride != size:\n",
        "        overlapping = (size // stride) - 1\n",
        "\n",
        "    if img_arr.ndim == 3:\n",
        "        i_max = img_arr.shape[0] // stride - overlapping\n",
        "        j_max = img_arr.shape[1] // stride - overlapping\n",
        "\n",
        "        for i in range(i_max):\n",
        "            for j in range(j_max):\n",
        "                # print(i*stride, i*stride+size)\n",
        "                # print(j*stride, j*stride+size)\n",
        "                patches_list.append(\n",
        "                    img_arr[\n",
        "                        i * stride : i * stride + size,\n",
        "                        j * stride : j * stride + size\n",
        "                    ]\n",
        "                )\n",
        "\n",
        "    elif img_arr.ndim == 4:\n",
        "        i_max = img_arr.shape[1] // stride - overlapping\n",
        "        for im in img_arr:\n",
        "            for i in range(i_max):\n",
        "                for j in range(i_max):\n",
        "                    # print(i*stride, i*stride+size)\n",
        "                    # print(j*stride, j*stride+size)\n",
        "                    patches_list.append(\n",
        "                        im[\n",
        "                            i * stride : i * stride + size,\n",
        "                            j * stride : j * stride + size,\n",
        "                        ]\n",
        "                    )\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"img_arr.ndim must be equal 3 or 4\")\n",
        "\n",
        "    return np.stack(patches_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOfFzQxzhm0i"
      },
      "source": [
        "###Â Code for creating datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP8y2bnyaXH7"
      },
      "source": [
        "import os, math\n",
        "from PIL import Image\n",
        "\n",
        "class CustomDataset(tf.keras.utils.Sequence):\n",
        "  def __init__(self, dataset_dir, which_subset, team, crop_type, img_generator=None,\n",
        "               preprocessing_function=None, validation_split=0.1, out_shape=[256, 256]):\n",
        "\n",
        "    self.crop_dir = os.path.join(dataset_dir, team, crop_type)\n",
        "    self.subset_filenames = os.listdir(os.path.join(self.crop_dir, 'Images'))\n",
        "\n",
        "    self.which_subset = which_subset\n",
        "    self.dataset_dir = dataset_dir\n",
        "    self.team = team\n",
        "    self.crop_type = crop_type\n",
        "    self.img_generator = img_generator\n",
        "    self.preprocessing_function = preprocessing_function\n",
        "    self.validation_split = validation_split\n",
        "    self.out_shape = out_shape\n",
        "\n",
        "  def __len__(self):\n",
        "    if self.which_subset == 'training':\n",
        "      return len(self.subset_filenames) - math.floor(len(self.subset_filenames) * self.validation_split)\n",
        "    return math.floor(len(self.subset_filenames) * self.validation_split)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    if self.which_subset == 'training':\n",
        "      curr_filename = self.subset_filenames[index]\n",
        "    else:\n",
        "      valid_delta = len(self.subset_filenames) - math.floor(len(self.subset_filenames) * self.validation_split) - 1\n",
        "      curr_filename = self.subset_filenames[index + valid_delta]\n",
        "\n",
        "    img = Image.open(os.path.join(self.crop_dir, 'Images', curr_filename))\n",
        "    mask = Image.open(os.path.join(self.crop_dir, 'Masks', os.path.splitext(curr_filename)[0] + '.png'))\n",
        "\n",
        "    img = img.resize(self.out_shape)\n",
        "    mask = mask.resize(self.out_shape)\n",
        "    \n",
        "    img_arr = np.array(img)\n",
        "    mask_arr = np.array(mask)\n",
        "\n",
        "    new_mask_arr = np.zeros(mask_arr.shape[:2], dtype=mask_arr.dtype)\n",
        "    new_mask_arr[np.where(np.all(mask_arr == [0, 0, 0], axis=-1))] = 0\n",
        "    new_mask_arr[np.where(np.all(mask_arr == [216, 124, 18], axis=-1))] = 0\n",
        "    new_mask_arr[np.where(np.all(mask_arr == [255, 255, 255], axis=-1))] = 1\n",
        "    new_mask_arr[np.where(np.all(mask_arr == [216, 67, 82], axis=-1))] = 2\n",
        "\n",
        "    mask_arr = np.expand_dims(new_mask_arr, -1)\n",
        "\n",
        "    if self.which_subset == 'training':\n",
        "      if self.img_generator is not None:\n",
        "        # Perform data augmentation\n",
        "        # We can get a random transformation from the ImageDataGenerator using get_random_transform\n",
        "        # and we can apply it to the image using apply_transform\n",
        "        img_t = self.img_generator.get_random_transform(img_arr.shape)\n",
        "        img_arr = self.img_generator.apply_transform(img_arr, img_t)\n",
        "        # ImageDataGenerator use bilinear interpolation for augmenting the images.\n",
        "        # Thus, when applied to the masks it will output 'interpolated classes', which\n",
        "        # is an unwanted behaviour. As a trick, we can transform each class mask \n",
        "        # separately and then we can cast to integer values (as in the binary segmentation notebook).\n",
        "        # Finally, we merge the augmented binary masks to obtain the final segmentation mask.\n",
        "        out_mask = np.zeros_like(mask_arr)\n",
        "        for c in np.unique(mask_arr):\n",
        "          if c > 0:\n",
        "            curr_class_arr = np.float32(mask_arr == c)\n",
        "            curr_class_arr = self.img_generator.apply_transform(curr_class_arr, img_t)\n",
        "            # from [0, 1] to {0, 1}\n",
        "            curr_class_arr = np.uint8(curr_class_arr)\n",
        "            # recover original class\n",
        "            curr_class_arr = curr_class_arr * c \n",
        "            out_mask += curr_class_arr\n",
        "      else:\n",
        "        out_mask = mask_arr\n",
        "    else:\n",
        "      out_mask = mask_arr\n",
        "\n",
        "    # One hot encoding\n",
        "    out_mask = tf.keras.utils.to_categorical(out_mask, num_classes=num_classes)\n",
        "\n",
        "    if self.preprocessing_function is not None:\n",
        "        img_arr = self.preprocessing_function(img_arr)\n",
        "\n",
        "    return img_arr, np.float32(out_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTNuqLgITkC4"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def GetDataset(dir, team, crop_type, apply_data_augmentation, validation_split, batch_size, preprocessing_function):\n",
        "  if apply_data_augmentation:\n",
        "    img_data_gen = ImageDataGenerator(rotation_range=20,\n",
        "                                      width_shift_range=10,\n",
        "                                      height_shift_range=10,\n",
        "                                      zoom_range=0.2,\n",
        "                                      rescale=1./255,\n",
        "                                      horizontal_flip=True,\n",
        "                                      vertical_flip=True,\n",
        "                                      fill_mode='reflect')\n",
        "  else:\n",
        "    img_data_gen = None\n",
        "\n",
        "  dataset = CustomDataset(dir, 'training', \n",
        "                          team, crop_type,\n",
        "                          img_generator=img_data_gen, \n",
        "                          validation_split=validation_split,\n",
        "                          preprocessing_function=preprocessing_function,\n",
        "                          out_shape=[img_h, img_w])\n",
        "  dataset_valid = CustomDataset(dir, 'validation', \n",
        "                                team, crop_type,\n",
        "                                img_generator=img_data_gen,\n",
        "                                validation_split=validation_split,\n",
        "                                preprocessing_function=preprocessing_function,\n",
        "                                out_shape=[img_h, img_w])\n",
        "\n",
        "  train_dataset = tf.data.Dataset.from_generator(lambda: dataset,\n",
        "                                                output_types=(tf.float32, tf.float32),\n",
        "                                                output_shapes=([img_h, img_w, 3], [img_h, img_w, num_classes]))\n",
        "  train_dataset = train_dataset.batch(batch_size)\n",
        "  train_dataset = train_dataset.repeat()\n",
        "\n",
        "  valid_dataset = tf.data.Dataset.from_generator(lambda: dataset_valid,\n",
        "                                                output_types=(tf.float32, tf.float32),\n",
        "                                                output_shapes=([img_h, img_w, 3], [img_h, img_w, num_classes]))\n",
        "  valid_dataset = valid_dataset.batch(batch_size)\n",
        "  valid_dataset = valid_dataset.repeat()\n",
        "\n",
        "  return train_dataset, dataset, valid_dataset, dataset_valid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_oM46Q3hsF9"
      },
      "source": [
        "### Code for saving testset results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ba1WBpQawed"
      },
      "source": [
        "import ntpath\n",
        "from PIL import Image\n",
        "\n",
        "sizes = {'Bipbip': [1536, 2048], 'Pead': [2464, 3280], 'Roseau': [819, 1227], 'Weedelec': [3456, 5184]}\n",
        "\n",
        "def PredictDatasets(file, model):\n",
        "  submission_dict = {}\n",
        "  for team in teams:\n",
        "    for crop_type in crop_types:\n",
        "      test_img_generator = ImageDataGenerator()\n",
        "      test_gen = test_img_generator.flow_from_directory(os.path.join('/content/Test_Final',\n",
        "                                        team, \n",
        "                                        crop_type),\n",
        "                                        batch_size=1,\n",
        "                                        target_size=(img_h, img_w),\n",
        "                                        classes=['Images'],\n",
        "                                        shuffle=False)\n",
        "\n",
        "      predictions = model.predict(test_gen, len(test_gen), verbose=1)\n",
        "      filenames = test_gen.filenames\n",
        "\n",
        "      i = 0\n",
        "      for p in predictions:\n",
        "        p = tf.image.resize(p, sizes[team], method='nearest').numpy()\n",
        "        mask_arr = np.argmax(p, -1)\n",
        "        img_name = os.path.splitext(ntpath.basename(filenames[i]))[0]\n",
        "        submission_dict[img_name] = {}\n",
        "        submission_dict[img_name]['shape'] = sizes[team]\n",
        "        submission_dict[img_name]['team'] = team\n",
        "        submission_dict[img_name]['crop'] = crop_type\n",
        "        submission_dict[img_name]['segmentation'] = {}\n",
        "\n",
        "        rle_encoded_crop = rle_encode(mask_arr == 1)\n",
        "        rle_encoded_weed = rle_encode(mask_arr == 2)\n",
        "\n",
        "        submission_dict[img_name]['segmentation']['crop'] = rle_encoded_crop\n",
        "        submission_dict[img_name]['segmentation']['weed'] = rle_encoded_weed\n",
        "        i = i + 1\n",
        "  \n",
        "  with open(file, 'w') as f:\n",
        "    json.dump(submission_dict, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8-A3vXmg-7b"
      },
      "source": [
        "# U-Net\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gteg6ElCplt4"
      },
      "source": [
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras import layers\n",
        "import ntpath\n",
        "from keras_unet.models import custom_unet\n",
        "\n",
        "def GetUnetModel():\n",
        "  inputs = tf.keras.Input(shape=(img_w, img_h) + (3,))\n",
        "\n",
        "  ### [First half of the network: downsampling inputs] ###\n",
        "\n",
        "  # Entry block\n",
        "  x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "  previous_block_activation = x  # Set aside residual\n",
        "\n",
        "  # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
        "  for filters in [64, 128, 256]:\n",
        "      x = layers.Activation(\"relu\")(x)\n",
        "      x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
        "      x = layers.BatchNormalization()(x)\n",
        "\n",
        "      x = layers.Activation(\"relu\")(x)\n",
        "      x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
        "      x = layers.BatchNormalization()(x)\n",
        "\n",
        "      x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "      # Project residual\n",
        "      residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(previous_block_activation)\n",
        "      x = layers.add([x, residual])  # Add back residual\n",
        "      previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "  ### [Second half of the network: upsampling inputs] ###\n",
        "\n",
        "  for filters in [256, 128, 64, 32]:\n",
        "      x = layers.Activation(\"relu\")(x)\n",
        "      x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "      x = layers.BatchNormalization()(x)\n",
        "\n",
        "      x = layers.Activation(\"relu\")(x)\n",
        "      x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "      x = layers.BatchNormalization()(x)\n",
        "\n",
        "      x = layers.UpSampling2D(2)(x)\n",
        "\n",
        "      # Project residual\n",
        "      residual = layers.UpSampling2D(2)(previous_block_activation)\n",
        "      residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
        "      x = layers.add([x, residual])  # Add back residual\n",
        "      previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "  # Add a per-pixel classification layer\n",
        "  outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
        "\n",
        "  # Define the model\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "  return model\n",
        "\n",
        "def GetCustomUnetModel():\n",
        "  model = custom_unet(input_shape=(img_w, img_h, 3),\n",
        "                      use_batch_norm=True,\n",
        "                      num_classes=num_classes,\n",
        "                      filters=64,\n",
        "                      dropout=0.3,\n",
        "                      output_activation='sigmoid')\n",
        "\n",
        "  return model\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "def GetModelFor(team, crop_type, custom):\n",
        "  #@markdown Set hyperparameters used during training\n",
        "\n",
        "  data_augmentation = True#@param {type:'boolean'}\n",
        "  validation_split = 0.1#@param {type:'number'}\n",
        "  batch_size = 1#@param {type:'number'}\n",
        "  epochs = 70#@param {type:'number'}\n",
        "  use_early_stopping = True#@param {type:'boolean'}\n",
        "\n",
        "  wandb.init(project='codalab2', config={\n",
        "      'team': team,\n",
        "      'crop_type': crop_type,\n",
        "      'custom': custom,\n",
        "      'data_augmentation': data_augmentation,\n",
        "      'validation_split': validation_split,\n",
        "      'batch_size': batch_size,\n",
        "      'epochs': epochs,\n",
        "      'use_early_stopping': use_early_stopping\n",
        "    })\n",
        "\n",
        "  datasets = GetDataset('/content/Development_Dataset/Training',\n",
        "                        team,\n",
        "                        crop_type,\n",
        "                        data_augmentation,\n",
        "                        validation_split,\n",
        "                        batch_size,\n",
        "                        None)\n",
        "  datasets_new = GetDataset('/content/Test_Dev',\n",
        "                            team,\n",
        "                            crop_type,\n",
        "                            data_augmentation,\n",
        "                            validation_split,\n",
        "                            1,\n",
        "                            None)\n",
        "  \n",
        "  if custom:\n",
        "    model = GetCustomUnetModel()\n",
        "  else:\n",
        "    model = GetUnetModel()\n",
        "\n",
        "  callbacks = [WandbCallback(data_type='image', \n",
        "                            log_weights=True,\n",
        "                            input_type='image',\n",
        "                            output_type='segmentation_mask',\n",
        "                            class_colors=class_colors)]\n",
        "  callbacks.append(tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                   factor=0.1,\n",
        "                                   patience=4,\n",
        "                                   cooldown=1,\n",
        "                                   min_delta=0,\n",
        "                                   verbose=1))                       \n",
        "                          \n",
        "  if use_early_stopping:\n",
        "    callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=8, restore_best_weights=True))\n",
        "  \n",
        "  model.compile(optimizer='rmsprop', \n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy', tf.keras.metrics.MeanIoU(num_classes)])\n",
        "\n",
        "  model.fit(x=datasets[0],\n",
        "            epochs=epochs,\n",
        "            steps_per_epoch=len(datasets[1]) // batch_size,\n",
        "            validation_data=datasets[2],\n",
        "            validation_steps=len(datasets[3]) // batch_size, \n",
        "            callbacks=callbacks)\n",
        "  !echo \"Train on final...\"\n",
        "  model.fit(x=datasets_new[0],\n",
        "            epochs=epochs,\n",
        "            steps_per_epoch=len(datasets_new[1]),\n",
        "            validation_data=datasets_new[2],\n",
        "            validation_steps=len(datasets_new[3]), \n",
        "            callbacks=callbacks)\n",
        "  \n",
        "  return model\n",
        "\n",
        "model = GetModelFor('Bipbip', 'Haricot', False)\n",
        "PredictDatasets('/content/drive/MyDrive/Colab Notebooks/CodaLab2/submission_final.json',\n",
        "                model)\n",
        "\n",
        "custom_model = GetModelFor('Bipbip', 'Haricot', True)\n",
        "PredictDatasets('/content/drive/MyDrive/Colab Notebooks/CodaLab2/submission_c_final.json',\n",
        "                custom_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtVrF-mxCXgO"
      },
      "source": [
        "### Results\n",
        "\n",
        "https://wandb.ai/lrsb/codalab2/runs/2n7mtkb9 (Score on Bipbip Haricot: 0.6443)\n",
        "\n",
        "https://wandb.ai/lrsb/codalab2/runs/31hoh2e5 (Score on Bipbip Haricot: 0.6149)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJiNBNtbsBKB"
      },
      "source": [
        "# Utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT5Q3kmmMXTC"
      },
      "source": [
        "### Display results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYRVFg7yMY0c"
      },
      "source": [
        "with open(os.path.join('/content/drive/MyDrive/Colab Notebooks/CodaLab2', 'submission.json'), 'r') as f:\n",
        "  submission_dict = json.load(f)\n",
        "  img_name = 'Bipbip_mais_im_04121'\n",
        "  img_shape = submission_dict[img_name]['shape']\n",
        "\n",
        "  rle_encoded_crop = submission_dict[img_name]['segmentation']['crop']\n",
        "  rle_encoded_weed = submission_dict[img_name]['segmentation']['weed']\n",
        "\n",
        "  # Reconstruct crop and weed binary masks\n",
        "  crop_mask = rle_decode(rle_encoded_crop, shape=img_shape)\n",
        "  weed_mask = rle_decode(rle_encoded_weed, shape=img_shape)\n",
        "\n",
        "  # Reconstruct original mask\n",
        "  # weed_mask * 2 allows to convert ones into target 2 (weed label)\n",
        "  reconstructed_mask = crop_mask + (weed_mask * 2)\n",
        "\n",
        "\n",
        "  # Just for visualisation purposes, save RGB reconstructed mask\n",
        "  # Use again the dictionary in 'RGBtoTarget.txt'.\n",
        "  reconstructed_rgb_arr = np.zeros(shape=img_shape + [3])\n",
        "  reconstructed_rgb_arr[reconstructed_mask == 1] = [255, 255, 255]\n",
        "  reconstructed_rgb_arr[reconstructed_mask == 2] = [216, 67, 82]\n",
        "\n",
        "  reconstructed_rgb_img = Image.fromarray(\n",
        "      np.uint8(reconstructed_rgb_arr))\n",
        "  \n",
        "  display(reconstructed_rgb_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcLfLMv_RZY3"
      },
      "source": [
        "### Test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnihNYhEFtLi"
      },
      "source": [
        "import time\n",
        "from matplotlib import cm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "valid_dataset = GetDataset('/content/Development_Dataset/Training', 'Bipbip', 'Haricot', True, 0, 1, None)\n",
        "\n",
        "iterator = iter(valid_dataset[0])\n",
        "\n",
        "fig, ax = plt.subplots(3, 12, figsize=(30, 8))\n",
        "\n",
        "for i in range(0, 3):\n",
        "  for j in range(0, 4):\n",
        "    augmented_img, target = next(iterator)\n",
        "    augmented_img = augmented_img[0]   # First element\n",
        "    prediction = model.predict(x=np.expand_dims(augmented_img, axis=0))\n",
        "\n",
        "    target = tf.expand_dims(tf.argmax(target, -1), -1)\n",
        "    prediction = tf.argmax(prediction, -1)[0]\n",
        "\n",
        "    prediction_img = np.zeros([prediction.shape[0], prediction.shape[1], 3])\n",
        "    for k in range(0, num_classes):\n",
        "      prediction_img[np.where(prediction == k)] = class_colors[k]\n",
        "\n",
        "    target = np.array(target[0, ..., 0])   # First element (squeezing channel dimension)\n",
        "\n",
        "    target_img = np.zeros([target.shape[0], target.shape[1], 3])\n",
        "    for k in range(0, num_classes):\n",
        "      target_img[np.where(target == k)] = class_colors[k]\n",
        "\n",
        "    ax[i][j].imshow(np.uint8(augmented_img))\n",
        "    ax[i][4+j].imshow(np.uint8(target_img))\n",
        "    ax[i][8+j].imshow(np.uint8(prediction_img))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Mmkb1zCm5no"
      },
      "source": [
        "### Predict dataset using saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPf2Yys8m1WF"
      },
      "source": [
        "dependencies = {\n",
        "    'meanIoU': meanIoU\n",
        "}\n",
        "\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/CodaLab2/model-best.h5', custom_objects=dependencies)\n",
        "\n",
        "PredictDatasets('/content/drive/MyDrive/Colab Notebooks/CodaLab2/submission.json', model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}